{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX0t5yFCNgMH",
        "outputId": "c76bfafc-ed27-4655-ded9-557db51cb6d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "# 1. Класс для загрузки и обработки OPUS100\n",
        "class OPUS100Dataset(Dataset):\n",
        "    def __init__(self, tokenizer, lang_pair=\"en-ru\", split=\"train\", max_length=128, num_samples=100):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Загрузка датасета OPUS100\n",
        "        dataset = load_dataset(\"opus100\", lang_pair, split=split)\n",
        "\n",
        "        # Ограничение количества примеров\n",
        "        if num_samples is not None:\n",
        "            dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
        "\n",
        "        # Подготовка данных в формате диалога\n",
        "        self.examples = []\n",
        "        for example in dataset:\n",
        "            self.examples.append({\n",
        "                'en': example['translation']['en'],\n",
        "                'ru': example['translation']['ru']\n",
        "            })\n",
        "\n",
        "        print(f\"Загружено {len(self.examples)} примеров перевода {lang_pair}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples[idx]\n",
        "        text = f\"Переведи на русский: {example['en']} ||| {example['ru']}\"\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': encoding['input_ids'].flatten()\n",
        "        }\n",
        "\n",
        "# 2. Загрузка предобученной модели\n",
        "def load_pretrained_model(model_name=\"gpt2\"):\n",
        "    print(f\"Загрузка модели {model_name}...\")\n",
        "    try:\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        print(f\"Модель {model_name} успешно загружена!\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка загрузки модели: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# 3. Тонкая настройка на OPUS100\n",
        "def fine_tune(model, tokenizer, output_dir=\"./opus100_finetuned\"):\n",
        "    # Подготовка данных\n",
        "    train_dataset = OPUS100Dataset(tokenizer, num_samples=100)\n",
        "\n",
        "    # Параметры обучения\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        save_steps=500,\n",
        "        save_total_limit=2,\n",
        "        learning_rate=5e-5,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=100,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        eval_steps=500,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    # Обучение\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "    )\n",
        "\n",
        "    print(\"Начало тонкой настройки на OPUS100...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Сохранение модели\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"Модель сохранена в {output_dir}\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# 4. Генерация ответов\n",
        "def generate_translation(model, tokenizer, prompt, max_length=100):\n",
        "    input_text = f\"Переведи на русский: {prompt} |||\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors='pt',\n",
        "        max_length=128,\n",
        "        truncation=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_length=max_length,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.2,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    # Извлекаем только перевод (часть после |||)\n",
        "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    translation = full_text.split(\"|||\")[-1].strip()\n",
        "\n",
        "    return translation\n",
        "\n",
        "# 5. Интерактивный режим перевода\n",
        "def interactive_translation(model, tokenizer):\n",
        "    print(\"\\nРежим перевода с английского на русский. Введите 'выход' для завершения.\")\n",
        "    while True:\n",
        "        text = input(\"Введите английский текст: \")\n",
        "        if text.lower() in ['выход', 'exit', 'quit']:\n",
        "            break\n",
        "\n",
        "        translation = generate_translation(model, tokenizer, text)\n",
        "        print(f\"Перевод: {translation}\\n\")\n",
        "\n",
        "# Основная функция\n",
        "def main():\n",
        "    # Загрузка предобученной модели\n",
        "    model, tokenizer = load_pretrained_model(\"gpt2-medium\")  # Можно использовать \"gpt2\", \"gpt2-large\"\n",
        "\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    # Тонкая настройка (опционально)\n",
        "    if input(\"Выполнить тонкую настройку на OPUS100? (y/n): \").lower() == 'y':\n",
        "        model, tokenizer = fine_tune(model, tokenizer)\n",
        "\n",
        "    # Запуск интерактивного перевода\n",
        "    interactive_translation(model, tokenizer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8A2QX9oPOwg",
        "outputId": "cf69ffe9-344d-46ca-9362-0e385d94c134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загрузка модели gpt2-medium...\n",
            "Модель gpt2-medium успешно загружена!\n",
            "Выполнить тонкую настройку на OPUS100? (y/n): n\n",
            "\n",
            "Режим перевода с английского на русский. Введите 'выход' для завершения.\n",
            "Введите английский текст: hello\n",
            "Перевод: |- - -||\n",
            " Tutorials in Russian, English and German are available at www.youtube . To get your free trial visit http://www of google (http:// Google ) or search for \"YouTube\" , select the video you wish to watch from here then click on play button as shown below...\n",
            "\n",
            "Введите английский текст: hi\n",
            "Перевод: |номотым||\n",
            "1. My mom and I went to the museum with my dad on our way back from school one day, we saw this amazing painting of a guy in his bathtub that had been painted by an artist named Sergei Prokofiev (1909-1980), who was also known as \"The Russian Master Painter\". The\n",
            "\n",
            "Введите английский текст: переведи этот текст. hi\n",
            "Перевод: > \"There is nothing wrong with being a man\".|| |<\"I have been in love for years.\"|| >\n",
            "\n",
            "RAW Paste Data\n",
            "\n",
            "Введите английский текст: kfbzfngudfh\n",
            "Перевод: |[](/kfb)||http://www.reddit!at/?utm_source=rpicas&utm%3Ano-click%%0B6a4c5de7e1d\n",
            " (or you can just click the link and see what it says, I know that some of us are lazy\n",
            "\n",
            "Введите английский текст: привет\n",
            "Перевод: |\n",
            " [5] The first half of the song is about a man who was taken from his family when he's 16 years old. He has to go back home, and then it goes on with this sad story where there are many people around him dying because of violence in their lives, or by abuse; for example rape (which\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
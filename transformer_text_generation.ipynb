{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class TransformerTextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len=100):\n",
        "        super(TransformerTextGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def prepare_data(text, seq_length=30):\n",
        "    words = text.split()\n",
        "    word_counts = Counter(words)\n",
        "    vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "    idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "    sequences = []\n",
        "    for i in range(seq_length, len(words)):\n",
        "        seq = words[i-seq_length:i+1]\n",
        "        sequences.append([word_to_idx[word] for word in seq])\n",
        "\n",
        "    return np.array(sequences), word_to_idx, idx_to_word, len(vocab)\n",
        "\n",
        "def train_model_transformer(model, sequences, vocab_size, seq_length=30, epochs=100, lr=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        for sequence in sequences:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Подготовка входных данных и целевых значений\n",
        "            input_seq = torch.tensor(sequence[:-1]).unsqueeze(0)  # [1, seq_len-1]\n",
        "            target = torch.tensor(sequence[1:])  # [seq_len-1]\n",
        "\n",
        "            # Создаем маску для автогрессии\n",
        "            src_mask = generate_square_subsequent_mask(input_seq.size(1))\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(input_seq, src_mask)  # [1, seq_len-1, vocab_size]\n",
        "\n",
        "            # Изменяем размерности для вычисления loss\n",
        "            output = output.view(-1, vocab_size)  # [(seq_len-1), vocab_size]\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "        scheduler.step()\n",
        "        avg_loss = total_loss / batch_count\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "            # Тест генерации\n",
        "            if avg_loss < 6.0:  # Когда модель немного обучилась\n",
        "                test_text = generate_text_transformer(model, \"машинное обучение\", word_to_idx, idx_to_word, seq_length, 10)\n",
        "                print(f\"Тест генерации: {test_text}\")\n",
        "\n",
        "def generate_text_transformer(model, start_text, word_to_idx, idx_to_word, seq_length, num_words=20, temperature=0.8):\n",
        "    model.eval()\n",
        "    words = start_text.split()\n",
        "\n",
        "    if len(words) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # Оставляем только слова из словаря\n",
        "    words = [word for word in words if word in word_to_idx]\n",
        "    if len(words) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # Если слов меньше seq_length, дополняем padding'ом\n",
        "    if len(words) < seq_length:\n",
        "        padded_words = ['<PAD>'] * (seq_length - len(words)) + words\n",
        "    else:\n",
        "        padded_words = words[-seq_length:]\n",
        "\n",
        "    generated_text = words.copy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        current_sequence = [word_to_idx[word] for word in padded_words]\n",
        "\n",
        "        for _ in range(num_words):\n",
        "            input_seq = torch.tensor(current_sequence).unsqueeze(0)  # [1, seq_length]\n",
        "\n",
        "            # Создаем маску\n",
        "            src_mask = generate_square_subsequent_mask(input_seq.size(1))\n",
        "\n",
        "            # Получаем предсказания\n",
        "            output = model(input_seq, src_mask)  # [1, seq_length, vocab_size]\n",
        "\n",
        "            # Берем предсказание для последнего слова в последовательности\n",
        "            next_word_logits = output[0, -1, :]  # [vocab_size]\n",
        "\n",
        "            # Применяем temperature\n",
        "            next_word_probs = torch.softmax(next_word_logits / temperature, dim=0)\n",
        "\n",
        "            # Выбираем следующее слово\n",
        "            next_word_idx = torch.multinomial(next_word_probs, 1).item()\n",
        "            next_word = idx_to_word.get(next_word_idx, \"<UNK>\")\n",
        "\n",
        "            # Обновляем последовательность\n",
        "            generated_text.append(next_word)\n",
        "            current_sequence = current_sequence[1:] + [next_word_idx]\n",
        "\n",
        "    return ' '.join(generated_text)\n",
        "\n",
        "# Пример использования\n",
        "text = \"\"\"машинное обучение это интересная область искусственного интеллекта\n",
        "которая позволяет компьютерам обучаться на данных и делать прогнозы\n",
        "нейронные сети являются мощным инструментом для решения сложных задач\n",
        "глубокое обучение использует многослойные нейронные сети\n",
        "для распознавания образов и обработки естественного языка\"\"\"\n",
        "\n",
        "print(\"Подготовка данных...\")\n",
        "sequences, word_to_idx, idx_to_word, vocab_size = prepare_data(text, seq_length=20)\n",
        "print(f\"Размер словаря: {vocab_size}\")\n",
        "print(f\"Количество последовательностей: {len(sequences)}\")\n",
        "\n",
        "# Добавляем токен паддинга в словарь\n",
        "word_to_idx['<PAD>'] = len(word_to_idx)\n",
        "idx_to_word[len(idx_to_word)] = '<PAD>'\n",
        "vocab_size += 1\n",
        "\n",
        "# Создаем модель Transformer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TransformerTextGenerator(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=128,\n",
        "    nhead=8,\n",
        "    num_layers=3,\n",
        "    max_len=50\n",
        ").to(device)\n",
        "\n",
        "print(f\"Модель создана на устройстве: {device}\")\n",
        "print(f\"Количество параметров: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "print(\"Начало обучения Transformer...\")\n",
        "train_model_transformer(model, sequences, vocab_size, seq_length=20, epochs=100, lr=0.001)\n",
        "\n",
        "# Финальная генерация\n",
        "print(\"\\nФинальная генерация:\")\n",
        "result = generate_text_transformer(model, \"машинное обучение\", word_to_idx, idx_to_word, 20, 25)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83uvqiLTB32W",
        "outputId": "df1f4367-e662-43f7-ae0a-3e8beb2a8e8c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Подготовка данных...\n",
            "Размер словаря: 33\n",
            "Количество последовательностей: 18\n",
            "Модель создана на устройстве: cpu\n",
            "Количество параметров: 1,787,810\n",
            "Начало обучения Transformer...\n",
            "Epoch 0, Loss: 0.9339, LR: 0.001000\n",
            "Тест генерации: машинное обучение использует многослойные нейронные сети являются мощным инструментом для естественного и\n",
            "Epoch 10, Loss: 0.0024, LR: 0.001000\n",
            "Тест генерации: машинное обучение использует многослойные нейронные сети являются мощным инструментом для решения сложных\n",
            "Epoch 20, Loss: 0.0011, LR: 0.001000\n",
            "Тест генерации: машинное обучение использует многослойные нейронные сети являются мощным инструментом для решения сложных\n",
            "Epoch 30, Loss: 0.0007, LR: 0.000100\n",
            "Тест генерации: машинное обучение использует многослойные нейронные сети являются мощным инструментом для решения сложных\n",
            "Epoch 40, Loss: 0.0006, LR: 0.000100\n",
            "Тест генерации: машинное обучение использует многослойные нейронные сети являются мощным инструментом для решения сложных\n",
            "Epoch 50, Loss: 0.0006, LR: 0.000100\n",
            "Тест генерации: машинное обучение использует многослойные нейронные сети являются мощным инструментом для решения сложных\n",
            "Epoch 60, Loss: 0.0006, LR: 0.000010\n",
            "Тест генерации: машинное обучение использует многослойные нейронные сети являются мощным инструментом для решения сложных\n",
            "Epoch 70, Loss: 0.0006, LR: 0.000010\n",
            "Тест генерации: машинное обучение использует многослойные нейронные сети являются мощным инструментом для решения сложных\n",
            "Epoch 80, Loss: 0.0006, LR: 0.000010\n",
            "Тест генерации: машинное обучение использует многослойные нейронные сети являются мощным инструментом для решения сложных\n",
            "Epoch 90, Loss: 0.0006, LR: 0.000001\n",
            "Тест генерации: машинное обучение использует многослойные нейронные сети являются мощным инструментом для решения сложных\n",
            "\n",
            "Финальная генерация:\n",
            "машинное обучение использует многослойные нейронные сети являются мощным инструментом для решения сложных задач глубокое обучение использует многослойные нейронные сети для распознавания образов и обработки естественного языка использует\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Без названия.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "jl6BJ-zgXyIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rcb_FEp7Znhm",
        "outputId": "0213ef47-fd33-4dce-8d15-20f114fc2fdd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-24 16:18:18--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-06-24 16:18:18 (18.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "y53KP1dUXMJ4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "dropout = 0.0\n",
        "learning_rate = 1e-3\n",
        "epochs = 10\n",
        "max_iters = 100\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "eval_interval = 10\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "\n",
        "with open('/content/input.txt', 'r', encoding='utf-8') as f:\n",
        "    x = f.read()\n",
        "\n",
        "class CharTokenizer:\n",
        "    def __init__(self, chars):\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.stoi = {ch: i for i, ch in enumerate(self.chars)}\n",
        "        self.itos = {i: ch for ch, i in self.stoi.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.stoi[ch] for ch in text]\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return ''.join([self.itos[token] for token in tokens])\n",
        "\n",
        "vocab = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,!?:;'\\n-&3$#@э!?*\")\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,d_model=128,max_len=16):\n",
        "    super().__init__()\n",
        "    self.pe = torch.zeros(1,max_len, d_model)\n",
        "    self.d_model = 128\n",
        "    self.max_len = 16\n",
        "    #self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "        # x: (B, T, D)\n",
        "    position = torch.arange(0, self.max_len).unsqueeze(1).float()\n",
        "    div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-np.log(10000.0) / self.d_model))\n",
        "    self.pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    self.pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    self.pe = self.pe.unsqueeze(0)\n",
        "    return self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model=128, max_len=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        returns: positional encoding (1, T, D)\n",
        "        \"\"\"\n",
        "        return self.pe[:, :x.size(1), :].to(x.device)\n",
        "\"\"\"def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\"\"\"\n",
        "\n",
        "\n",
        "class Feed_Forward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "  def forward(self,x):\n",
        "      return self.net(x)\n",
        "\n",
        "class Multi(nn.Module):\n",
        "      def __init__(self):\n",
        "        super().__init__()\n",
        "        d_model=128\n",
        "        num_heads=4\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "        self.W_A = nn.Linear(d_model, d_model)\n",
        "      def forward(self,Q,K,V):\n",
        "        batch_size = Q.size(0)\n",
        "        Q = self.W_Q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_K(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_V(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(weights, V)\n",
        "\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.W_A(output)\n",
        "\n",
        "class Bert(nn.Module):\n",
        "    def __init__(self, d_model=128, num_heads=4, block_size=16, vocab=vocab,vocab_size=len(vocab)):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding()\n",
        "\n",
        "        self.mha = Multi()\n",
        "        self.ff = Feed_Forward(d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.output = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        x1 = self.token_embedding(idx)  # (B, T, D)\n",
        "        x2 = self.pos_enc(x1)\n",
        "        x = x1 + x2\n",
        "        x = self.norm1(self.mha(x,x,x) + x)\n",
        "        x = self.norm2(self.ff(x) + x)\n",
        "        logits = self.output(x)\n",
        "        return logits\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = self.forward(idx)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, next_token], dim=1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = Bert().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(model, data, optimizer, criterion, epochs=30, batch_size=32, block_size=32, device='cpu'):\n",
        "    model.train()\n",
        "    count  = 0\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for _ in range(len(data) // batch_size):\n",
        "            ix = torch.randint(0, len(data) - block_size - 1, (batch_size,), device=device)\n",
        "            x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "            y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
        "           # print('y',y)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            #print('log',logits)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            count += 1\n",
        "         #   print(count)\n",
        "            if count > 1500:\n",
        "              break\n",
        "\n",
        "        avg_loss = total_loss / (len(data) // batch_size)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: loss={avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "yNHjsLN3OzWM"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,!?:;'\\n-&3$#@э!?*\")\n",
        "tokenizer = CharTokenizer(vocab)"
      ],
      "metadata": {
        "id": "P2LpFj2ZRuBt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = torch.tensor(tokenizer.encode(x[:100]), dtype=torch.long)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "IMge6xVBRfEl"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaNb-LAx3aJS",
        "outputId": "4f9a46d2-10df-498a-b79b-bba3cf8a5890"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    model = model,\n",
        "    data = tokens,\n",
        "    optimizer = optimizer,\n",
        "    criterion = criterion\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHuP3islPEKt",
        "outputId": "b28c2f2e-5fac-4083-be69-ea6f3f5f1690"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss=0.0010\n",
            "Epoch 2: loss=0.0003\n",
            "Epoch 3: loss=0.0040\n",
            "Epoch 4: loss=0.0071\n",
            "Epoch 5: loss=0.0036\n",
            "Epoch 6: loss=0.0002\n",
            "Epoch 7: loss=0.0002\n",
            "Epoch 8: loss=0.0002\n",
            "Epoch 9: loss=0.0001\n",
            "Epoch 10: loss=0.0001\n",
            "Epoch 11: loss=0.0002\n",
            "Epoch 12: loss=0.0002\n",
            "Epoch 13: loss=0.0001\n",
            "Epoch 14: loss=0.0001\n",
            "Epoch 15: loss=0.0001\n",
            "Epoch 16: loss=0.0001\n",
            "Epoch 17: loss=0.0001\n",
            "Epoch 18: loss=0.0000\n",
            "Epoch 19: loss=0.0001\n",
            "Epoch 20: loss=0.0001\n",
            "Epoch 21: loss=0.0000\n",
            "Epoch 22: loss=0.0000\n",
            "Epoch 23: loss=0.0000\n",
            "Epoch 24: loss=0.0000\n",
            "Epoch 25: loss=0.0000\n",
            "Epoch 26: loss=0.0000\n",
            "Epoch 27: loss=0.0000\n",
            "Epoch 28: loss=0.0000\n",
            "Epoch 29: loss=0.0000\n",
            "Epoch 30: loss=0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(model, tokenizer, prompt, max_new_tokens=50, device='cpu'):\n",
        "    model.eval()\n",
        "    input_ids = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0].tolist())\n",
        "    return output_text\n"
      ],
      "metadata": {
        "id": "7Z04WuBHYDXV"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"vocab = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,!?:;'\\n-&3$#@э!?*\")\n",
        "tokenizer = CharTokenizer(vocab)\n",
        "vocab_size = len(tokenizer.chars)\n",
        "print(vocab_size)\"\"\"\n",
        "#model = LlamaLike(vocab_size=vocab_size, d_model=128, num_heads=4, num_kv_groups=1)\n",
        "\n",
        "# пример запроса\n",
        "prompt = \"Before we processed\"\n",
        "response = chat(model, tokenizer, prompt, max_new_tokens=100)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03rzpYd73KrU",
        "outputId": "cbb33134-b4a3-41c3-8478-db72ff30e84d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before we processed             mi p I I E O BEN I es re wBedre Sss p  S s N  S k k N P OB Bo   e I SBes k S s k S Bsss\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class TextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
        "        super(TextGenerator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        output = self.fc(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        device = next(self.parameters()).device\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))\n",
        "\n",
        "def prepare_data(text, seq_length=10):  # Уменьшил длину последовательности\n",
        "    words = text.split()\n",
        "    word_counts = Counter(words)\n",
        "    vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "    idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "    sequences = []\n",
        "    for i in range(seq_length, len(words)):\n",
        "        seq = words[i-seq_length:i+1]\n",
        "        sequences.append([word_to_idx[word] for word in seq])\n",
        "\n",
        "    return np.array(sequences), word_to_idx, idx_to_word, len(vocab)\n",
        "\n",
        "def train_model(model, sequences, vocab_size, epochs=100, lr=0.01):  # Увеличил learning rate\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Переводим модель в режим обучения\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        hidden = model.init_hidden(1)  # Инициализируем hidden для каждой эпохи\n",
        "\n",
        "        for sequence in sequences:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Подготовка данных\n",
        "            input_seq = torch.tensor(sequence[:-1]).unsqueeze(0)  # [1, seq_len-1]\n",
        "            target = torch.tensor(sequence[1:])  # [seq_len-1]\n",
        "\n",
        "            # Forward pass\n",
        "            output, hidden = model(input_seq, hidden)\n",
        "\n",
        "            # Правильный расчет loss\n",
        "            output = output.squeeze(0)  # [seq_len-1, vocab_size]\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Градиентный clipping\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Детach и clone hidden states\n",
        "            hidden = (hidden[0].detach().clone(),\n",
        "                     hidden[1].detach().clone())\n",
        "\n",
        "        avg_loss = total_loss / len(sequences)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "            # Проверка генерации во время обучения\n",
        "            if avg_loss < 5.0:  # Только когда модель чему-то научилась\n",
        "                test_text = generate_text(model, \"машинное обучение\", word_to_idx, idx_to_word, 5)\n",
        "                print(f\"Тест генерации: {test_text}\")\n",
        "\n",
        "def generate_text(model, start_text, word_to_idx, idx_to_word, num_words=10, temperature=1.0):\n",
        "    model.eval()\n",
        "    words = start_text.split()\n",
        "\n",
        "    if len(words) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # Проверяем, что все слова есть в словаре\n",
        "    words = [word for word in words if word in word_to_idx]\n",
        "    if len(words) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    hidden = model.init_hidden(1)\n",
        "    generated_text = words.copy()\n",
        "\n",
        "    # Инициализация hidden state\n",
        "    with torch.no_grad():\n",
        "        for word in words[:-1]:\n",
        "            input_word = torch.tensor([[word_to_idx[word]]])\n",
        "            _, hidden = model(input_word, hidden)\n",
        "\n",
        "        current_word = words[-1]\n",
        "\n",
        "        for _ in range(num_words):\n",
        "            input_word = torch.tensor([[word_to_idx[current_word]]])\n",
        "            output, hidden = model(input_word, hidden)\n",
        "\n",
        "            # Получаем вероятности\n",
        "            output = output.squeeze(0).squeeze(0)\n",
        "            probabilities = torch.softmax(output / temperature, dim=0)\n",
        "\n",
        "            # Выбираем следующее слово\n",
        "            next_word_idx = torch.multinomial(probabilities, 1).item()\n",
        "            current_word = idx_to_word.get(next_word_idx, \"<UNK>\")\n",
        "\n",
        "            generated_text.append(current_word)\n",
        "\n",
        "    return ' '.join(generated_text)\n",
        "\n",
        "# Тестовый текст\n",
        "text = \"\"\"машинное обучение это интересная область искусственного интеллекта\n",
        "которая позволяет компьютерам обучаться на данных и делать прогнозы\n",
        "нейронные сети являются мощным инструментом для решения сложных задач\n",
        "глубокое обучение использует многослойные нейронные сети\n",
        "для распознавания образов и обработки естественного языка\"\"\"\n",
        "\n",
        "print(\"Подготовка данных...\")\n",
        "sequences, word_to_idx, idx_to_word, vocab_size = prepare_data(text, seq_length=5)\n",
        "print(f\"Размер словаря: {vocab_size}\")\n",
        "print(f\"Количество последовательностей: {len(sequences)}\")\n",
        "\n",
        "# Создаем модель\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TextGenerator(vocab_size, 64, 128).to(device)  # Уменьшил размерности\n",
        "\n",
        "print(\"Начало обучения...\")\n",
        "train_model(model, sequences, vocab_size, epochs=100, lr=0.01)\n",
        "\n",
        "# Финальная генерация\n",
        "print(\"\\nФинальная генерация:\")\n",
        "result = generate_text(model, \"машинное обучение\", word_to_idx, idx_to_word, 15)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8ASzu0dh_df",
        "outputId": "8af7e28f-0d31-4024-b9c6-cee59ec93510"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Подготовка данных...\n",
            "Размер словаря: 33\n",
            "Количество последовательностей: 33\n",
            "Начало обучения...\n",
            "Epoch 0, Loss: 3.0903\n",
            "Тест генерации: машинное обучение сети для сети распознавания использует\n",
            "Epoch 10, Loss: 0.0020\n",
            "Тест генерации: машинное обучение использует многослойные нейронные сети для\n",
            "Epoch 20, Loss: 0.0007\n",
            "Тест генерации: машинное обучение это интересная область искусственного интеллекта\n",
            "Epoch 30, Loss: 0.0004\n",
            "Тест генерации: машинное обучение это интересная область искусственного интеллекта\n",
            "Epoch 40, Loss: 0.0002\n",
            "Тест генерации: машинное обучение это интересная область искусственного интеллекта\n",
            "Epoch 50, Loss: 0.0002\n",
            "Тест генерации: машинное обучение это интересная область искусственного интеллекта\n",
            "Epoch 60, Loss: 0.0001\n",
            "Тест генерации: машинное обучение это интересная область искусственного интеллекта\n",
            "Epoch 70, Loss: 0.0001\n",
            "Тест генерации: машинное обучение это интересная область искусственного интеллекта\n",
            "Epoch 80, Loss: 0.0001\n",
            "Тест генерации: машинное обучение это интересная область искусственного интеллекта\n",
            "Epoch 90, Loss: 0.0000\n",
            "Тест генерации: машинное обучение это интересная область искусственного интеллекта\n",
            "\n",
            "Финальная генерация:\n",
            "машинное обучение это интересная область искусственного интеллекта которая позволяет компьютерам обучаться на данных и делать прогнозы нейронные\n"
          ]
        }
      ]
    }
  ]
}
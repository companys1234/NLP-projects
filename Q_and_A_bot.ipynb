{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# 1. Класс для загрузки датасета\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.conversations = []\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if '|||' in line:\n",
        "                    question, answer = line.strip().split('|||', 1)\n",
        "                    self.conversations.append((question.strip(), answer.strip()))\n",
        "\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.conversations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question, answer = self.conversations[idx]\n",
        "        text = f\"Вопрос: {question}\\nОтвет: {answer}\"\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': encoding['input_ids'].flatten()\n",
        "        }\n",
        "\n",
        "# 2. Загрузка модели и токенизатора из интернета\n",
        "def load_model_and_tokenizer(model_name=\"sberbank-ai/rugpt3small_based_on_gpt2\"):\n",
        "    print(f\"Загрузка модели {model_name}...\")\n",
        "    try:\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        print(\"Модель и токенизатор успешно загружены!\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка загрузки: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# 3. Тонкая настройка модели\n",
        "def fine_tune(model, tokenizer, dataset_path, output_dir=\"./fine_tuned_model\"):\n",
        "    # Проверка GPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    print(f\"Используется устройство: {device}\")\n",
        "\n",
        "    # Загрузка данных\n",
        "    train_dataset = ChatDataset(dataset_path, tokenizer)\n",
        "    print(f\"Загружено примеров: {len(train_dataset)}\")\n",
        "\n",
        "    # Параметры обучения\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=8,\n",
        "        per_device_train_batch_size=4,\n",
        "        save_steps=500,\n",
        "        save_total_limit=2,\n",
        "        learning_rate=5e-5,\n",
        "        logging_steps=100,\n",
        "        fp16=torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    # Обучение\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "    )\n",
        "\n",
        "    print(\"Начало обучения...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Сохранение модели\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"Модель сохранена в {output_dir}\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# 4. Генерация ответов\n",
        "def generate_response(model, tokenizer, prompt, max_length=50):\n",
        "    # Формируем промпт\n",
        "    input_text = f\"Вопрос: {prompt}\\nОтвет:\"\n",
        "\n",
        "    # Токенизация с явным указанием attention_mask\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors='pt',\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Генерация с \"холодным\" (более предсказуемым) алгоритмом\n",
        "    output = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_length=len(inputs.input_ids[0]) + max_length,\n",
        "        temperature=0.7,  # Уменьшаем \"креативность\"\n",
        "        top_k=30,         # Ограничиваем словарь при генерации\n",
        "        top_p=0.9,        # Nucleus sampling\n",
        "        repetition_penalty=1.5,  # Штраф за повторения\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    # Извлекаем только ответ (после \"Ответ:\")\n",
        "    full_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    answer = full_text.split(\"Ответ:\")[-1].strip()\n",
        "\n",
        "    # Ограничим ответ первым предложением\n",
        "    if '.' in answer:\n",
        "        answer = answer.split('.')[0] + '.'\n",
        "    elif '!' in answer:\n",
        "        answer = answer.split('!')[0] + '!'\n",
        "    elif '?' in answer:\n",
        "        answer = answer.split('?')[0] + '?'\n",
        "\n",
        "    return answer\n",
        "\n",
        "# 5. Основной цикл чата\n",
        "def run_chatbot():\n",
        "    # Загрузка модели (из интернета)\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    # Проверка необходимости тонкой настройки\n",
        "    if input(\"Выполнить тонкую настройку? (y/n): \").lower() == 'y':\n",
        "        dataset_path = input(\"Введите путь к файлу с данными (dialogues.txt): \")\n",
        "        model, tokenizer = fine_tune(model, tokenizer, dataset_path)\n",
        "\n",
        "    # Чат-режим\n",
        "    print(\"\\nЧат-бот готов! Введите 'выход' для завершения.\")\n",
        "    while True:\n",
        "        user_input = input(\"Вы: \")\n",
        "        if user_input.lower() in ['выход', 'exit', 'quit']:\n",
        "            break\n",
        "\n",
        "        response = generate_response(model, tokenizer, user_input)\n",
        "        print(f\"Бот: {response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "yOjTbj1W6wbt",
        "outputId": "0f5d9099-033b-40c2-d461-68fec5670499"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загрузка модели sberbank-ai/rugpt3small_based_on_gpt2...\n",
            "Модель и токенизатор успешно загружены!\n",
            "Выполнить тонкую настройку? (y/n): y\n",
            "Введите путь к файлу с данными (dialogues.txt): /content/q and a.txt\n",
            "Используется устройство: cpu\n",
            "Загружено примеров: 2\n",
            "Начало обучения...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 01:07, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Модель сохранена в ./fine_tuned_model\n",
            "\n",
            "Чат-бот готов! Введите 'выход' для завершения.\n",
            "Вы: привет\n",
            "Бот: если в тексте вопроса есть опечатка, то это ошибка.\n",
            "Вы: привет\n",
            "Бот: Привет! Не знаю как Вам, но я думаю что это не так.\n",
            "Вы: как дела?\n",
            "Бот: Как и все.\n",
            "Вы: выход\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_weights, v)\n",
        "\n",
        "        return output, attn_weights\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size, seq_len = q.size(0), q.size(1)\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.w_q(q).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.w_k(k).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.w_v(v).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Apply attention\n",
        "        attn_output, attn_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        # Concatenate heads\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, seq_len, self.d_model\n",
        "        )\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.w_o(attn_output)\n",
        "\n",
        "        return output, attn_weights\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Multi-head attention with residual connection and layer norm\n",
        "        attn_output, attn_weights = self.attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Feed-forward with residual connection and layer norm\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "        return x, attn_weights\n",
        "\n",
        "class QATransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, num_heads=8, num_layers=4,\n",
        "                 ff_dim=512, max_seq_len=512, dropout=0.1):\n",
        "        super(QATransformer, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Embedding layers\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def create_mask(self, seq):\n",
        "        \"\"\"Create causal mask for decoder\"\"\"\n",
        "        seq_len = seq.size(1)\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
        "        return mask.unsqueeze(0).unsqueeze(1)  # Add batch and head dimensions\n",
        "\n",
        "    def forward(self, x, return_attentions=False):\n",
        "        batch_size, seq_len = x.size()\n",
        "\n",
        "        # Create causal mask\n",
        "        mask = self.create_mask(x)\n",
        "\n",
        "        # Embedding + positional encoding\n",
        "        token_emb = self.token_embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.positional_encoding(token_emb.transpose(0, 1)).transpose(0, 1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        attentions = []\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for transformer in self.transformer_blocks:\n",
        "            x, attn_weights = transformer(x, mask)\n",
        "            if return_attentions:\n",
        "                attentions.append(attn_weights)\n",
        "\n",
        "        # Output projection\n",
        "        logits = self.output_layer(x)\n",
        "\n",
        "        if return_attentions:\n",
        "            return logits, attentions\n",
        "\n",
        "        return logits\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.inverse_vocab = {}\n",
        "        self.vocab_size = 0\n",
        "\n",
        "        # Специальные токены\n",
        "        self.special_tokens = {\n",
        "            '<pad>': 0,\n",
        "            '<unk>': 1,\n",
        "            '<bos>': 2,\n",
        "            '<eos>': 3,\n",
        "            '<sep>': 4  # Разделитель вопроса и ответа\n",
        "        }\n",
        "\n",
        "        # Инициализация специальными токенами\n",
        "        for token, idx in self.special_tokens.items():\n",
        "            self.vocab[token] = idx\n",
        "            self.inverse_vocab[idx] = token\n",
        "\n",
        "        self.vocab_size = len(self.special_tokens)\n",
        "\n",
        "    def build_vocab(self, texts, min_freq=1):\n",
        "        \"\"\"Построение словаря из текстов\"\"\"\n",
        "        counter = Counter()\n",
        "\n",
        "        for text in texts:\n",
        "            # Улучшенная токенизация\n",
        "            tokens = re.findall(r'\\w+|[^\\w\\s]', text.lower())\n",
        "            counter.update(tokens)\n",
        "\n",
        "        # Добавляем частые слова в словарь\n",
        "        for token, freq in counter.items():\n",
        "            if freq >= min_freq and token not in self.vocab:\n",
        "                self.vocab[token] = self.vocab_size\n",
        "                self.inverse_vocab[self.vocab_size] = token\n",
        "                self.vocab_size += 1\n",
        "\n",
        "    def encode(self, text, max_length=None, add_special_tokens=True):\n",
        "        \"\"\"Кодирование текста в индексы\"\"\"\n",
        "        tokens = re.findall(r'\\w+|[^\\w\\s]', text.lower())\n",
        "\n",
        "        if add_special_tokens:\n",
        "            indices = [self.special_tokens['<bos>']]\n",
        "        else:\n",
        "            indices = []\n",
        "\n",
        "        for token in tokens:\n",
        "            if token in self.vocab:\n",
        "                indices.append(self.vocab[token])\n",
        "            else:\n",
        "                indices.append(self.special_tokens['<unk>'])\n",
        "\n",
        "        if add_special_tokens:\n",
        "            indices.append(self.special_tokens['<eos>'])\n",
        "\n",
        "        if max_length:\n",
        "            if len(indices) < max_length:\n",
        "                indices = indices + [self.special_tokens['<pad>']] * (max_length - len(indices))\n",
        "            else:\n",
        "                indices = indices[:max_length]\n",
        "\n",
        "        return indices\n",
        "\n",
        "    def decode(self, indices, skip_special_tokens=True):\n",
        "        \"\"\"Декодирование индексов в текст\"\"\"\n",
        "        tokens = []\n",
        "        for idx in indices:\n",
        "            if idx in self.inverse_vocab:\n",
        "                token = self.inverse_vocab[idx]\n",
        "                if skip_special_tokens and token in self.special_tokens:\n",
        "                    continue\n",
        "                tokens.append(token)\n",
        "\n",
        "        # Восстанавливаем текст с пробелами\n",
        "        text = ' '.join(tokens)\n",
        "        # Убираем лишние пробелы вокруг знаков препинания\n",
        "        text = re.sub(r'\\s+([^\\w\\s])', r'\\1', text)\n",
        "        text = re.sub(r'([^\\w\\s])\\s+', r'\\1', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "class QADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, questions, answers, tokenizer, max_length=128):\n",
        "        self.questions = questions\n",
        "        self.answers = answers\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        answer = self.answers[idx]\n",
        "\n",
        "        # Форматируем: <bos> вопрос <sep> ответ <eos>\n",
        "        input_text = f\"{question} <sep> {answer}\"\n",
        "\n",
        "        # Токенизируем\n",
        "        input_ids = self.tokenizer.encode(input_text, self.max_length)\n",
        "\n",
        "        # Для языковой модели вход и цель одинаковы\n",
        "        target_ids = input_ids.copy()\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
        "            'target_ids': torch.tensor(target_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class QATrainer:\n",
        "    def __init__(self, model, tokenizer, device=None):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self, train_loader, val_loader=None, epochs=10, lr=0.001):\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer.special_tokens['<pad>'])\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            self.model.train()\n",
        "            total_train_loss = 0\n",
        "\n",
        "            for batch_idx, batch in enumerate(train_loader):\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                target_ids = batch['target_ids'].to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                logits = self.model(input_ids)\n",
        "\n",
        "                # Calculate loss - сдвигаем на 1 токен для языкового моделирования\n",
        "                loss = criterion(\n",
        "                    logits[:, :-1, :].contiguous().view(-1, self.model.vocab_size),\n",
        "                    target_ids[:, 1:].contiguous().view(-1)\n",
        "                )\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                if batch_idx % 10 == 0:\n",
        "                    print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "            train_losses.append(avg_train_loss)\n",
        "\n",
        "            # Validation\n",
        "            if val_loader:\n",
        "                avg_val_loss = self.validate(val_loader, criterion)\n",
        "                val_losses.append(avg_val_loss)\n",
        "                print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "            else:\n",
        "                print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "        return train_losses, val_losses\n",
        "\n",
        "    def validate(self, val_loader, criterion):\n",
        "        self.model.eval()\n",
        "        total_val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                target_ids = batch['target_ids'].to(self.device)\n",
        "\n",
        "                logits = self.model(input_ids)\n",
        "                loss = criterion(\n",
        "                    logits[:, :-1, :].contiguous().view(-1, self.model.vocab_size),\n",
        "                    target_ids[:, 1:].contiguous().view(-1)\n",
        "                )\n",
        "\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        return total_val_loss / len(val_loader)\n",
        "\n",
        "    def generate_answer(self, question, max_length=50, temperature=0.8):\n",
        "        \"\"\"Генерация ответа на вопрос\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        # Начинаем с вопроса и <sep>\n",
        "        input_text = f\"{question} <sep>\"\n",
        "        input_ids = self.tokenizer.encode(input_text, add_special_tokens=False)\n",
        "\n",
        "        # Добавляем <bos> в начало\n",
        "        input_ids = [self.tokenizer.special_tokens['<bos>']] + input_ids\n",
        "        generated = input_ids.copy()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "                # Подготавливаем вход\n",
        "                input_tensor = torch.tensor([generated], dtype=torch.long).to(self.device)\n",
        "\n",
        "                # Получаем логиты\n",
        "                logits = self.model(input_tensor)\n",
        "\n",
        "                # Берем логиты для последнего токена\n",
        "                next_token_logits = logits[0, -1, :] / temperature\n",
        "\n",
        "                # Применяем softmax для получения вероятностей\n",
        "                probs = torch.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "                # Сэмплируем следующий токен\n",
        "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "                # Добавляем токен к сгенерированной последовательности\n",
        "                generated.append(next_token)\n",
        "\n",
        "                # Если достигли <eos>, останавливаемся\n",
        "                if next_token == self.tokenizer.special_tokens['<eos>']:\n",
        "                    break\n",
        "\n",
        "        # Извлекаем только ответ (всё что после <sep>)\n",
        "        try:\n",
        "            sep_idx = generated.index(self.tokenizer.special_tokens['<sep>'])\n",
        "            answer_ids = generated[sep_idx + 1:]\n",
        "\n",
        "            # Убираем <eos> если есть\n",
        "            if self.tokenizer.special_tokens['<eos>'] in answer_ids:\n",
        "                eos_idx = answer_ids.index(self.tokenizer.special_tokens['<eos>'])\n",
        "                answer_ids = answer_ids[:eos_idx]\n",
        "        except ValueError:\n",
        "            # Если <sep> не найден, возвращаем всю сгенерированную последовательность\n",
        "            answer_ids = generated[len(input_ids):]\n",
        "\n",
        "        return self.tokenizer.decode(answer_ids)\n",
        "\n",
        "def create_sample_data():\n",
        "    \"\"\"Создание примеров данных для обучения\"\"\"\n",
        "    questions = [\n",
        "        \"какая столица франции\",\n",
        "        \"как работает фотосинтез\",\n",
        "        \"что такое искусственный интеллект\",\n",
        "        \"как приготовить пасту\",\n",
        "        \"какие планеты в солнечной системе\",\n",
        "        \"что такое python\",\n",
        "        \"как работает интернет\",\n",
        "        \"что такое машинное обучение\",\n",
        "        \"как сохранить здоровье\",\n",
        "        \"что такое черная дыра\"\n",
        "    ]\n",
        "\n",
        "    answers = [\n",
        "        \"столица франции париж\",\n",
        "        \"фотосинтез преобразует свет в энергию\",\n",
        "        \"ии это системы имитирующие человеческий интеллект\",\n",
        "        \"варите пасту в кипящей воде 10 минут\",\n",
        "        \"меркурий венера земля марс юпитер сатурн уран нептун\",\n",
        "        \"python это язык программирования высокого уровня\",\n",
        "        \"интернет это сеть соединенных компьютеров\",\n",
        "        \"мо это алгоритмы обучающиеся на данных\",\n",
        "        \"ешьте здоровую пищу и занимайтесь спортом\",\n",
        "        \"черная дыра это область с огромной гравитацией\"\n",
        "    ]\n",
        "\n",
        "    return questions, answers\n",
        "\n",
        "def main():\n",
        "    # Параметры\n",
        "    D_MODEL = 756  # Уменьшим для более быстрого обучения\n",
        "    NUM_HEADS = 4\n",
        "    NUM_LAYERS = 6\n",
        "    FF_DIM = 512\n",
        "    MAX_LENGTH = 64\n",
        "    BATCH_SIZE = 1\n",
        "    EPOCHS = 100\n",
        "\n",
        "    # Создание данных\n",
        "    questions, answers = create_sample_data()\n",
        "\n",
        "    # Инициализация токенизатора\n",
        "    tokenizer = SimpleTokenizer()\n",
        "\n",
        "    # Построение словаря\n",
        "    all_texts = questions + answers\n",
        "    tokenizer.build_vocab(all_texts, min_freq=1)\n",
        "\n",
        "    print(f\"Размер словаря: {tokenizer.vocab_size}\")\n",
        "    print(\"Пример словаря:\", dict(list(tokenizer.vocab.items())[:10]))\n",
        "\n",
        "    # Создание датасета и загрузчика\n",
        "    dataset = QADataset(questions, answers, tokenizer, MAX_LENGTH)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=BATCH_SIZE, shuffle=True\n",
        "    )\n",
        "\n",
        "    # Инициализация модели\n",
        "    model = QATransformer(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        d_model=D_MODEL,\n",
        "        num_heads=NUM_HEADS,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        ff_dim=FF_DIM,\n",
        "        max_seq_len=MAX_LENGTH\n",
        "    )\n",
        "\n",
        "    print(f\"Модель создана. Параметров: {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "    # Обучение\n",
        "    trainer = QATrainer(model, tokenizer)\n",
        "    print(\"Начало обучения...\")\n",
        "    train_losses, _ = trainer.train(train_loader, epochs=EPOCHS, lr=0.001)\n",
        "\n",
        "    # Тестирование\n",
        "    test_questions = [\n",
        "        \"какая столица франции\",\n",
        "        \"как работает фотосинтез\",\n",
        "        \"что такое черная дыра\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ТЕСТИРОВАНИЕ МОДЕЛИ\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for question in test_questions:\n",
        "        answer = trainer.generate_answer(question, temperature=0.5)\n",
        "        print(f\"В: {question}\")\n",
        "        print(f\"О: {answer}\\n\")\n",
        "\n",
        "    # Сохранение модели\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'tokenizer_vocab': tokenizer.vocab,\n",
        "        'tokenizer_inverse_vocab': tokenizer.inverse_vocab,\n",
        "        'model_config': {\n",
        "            'vocab_size': tokenizer.vocab_size,\n",
        "            'd_model': D_MODEL,\n",
        "            'num_heads': NUM_HEADS,\n",
        "            'num_layers': NUM_LAYERS,\n",
        "            'ff_dim': FF_DIM,\n",
        "            'max_seq_len': MAX_LENGTH\n",
        "        }\n",
        "    }, 'qa_model.pth')\n",
        "\n",
        "    print(\"Модель сохранена в 'qa_model.pth'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TulPLhzAneOd",
        "outputId": "9b2f8d02-f368-4a6c-f728-48674e63173a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер словаря: 74\n",
            "Пример словаря: {'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3, '<sep>': 4, 'какая': 5, 'столица': 6, 'франции': 7, 'как': 8, 'работает': 9}\n",
            "Модель создана. Параметров: 18517586\n",
            "Начало обучения...\n",
            "Epoch: 1, Batch: 0, Loss: 4.5798\n",
            "Epoch 1/100, Train Loss: 5.0639\n",
            "Epoch: 2, Batch: 0, Loss: 4.4153\n",
            "Epoch 2/100, Train Loss: 3.9020\n",
            "Epoch: 3, Batch: 0, Loss: 2.8333\n",
            "Epoch 3/100, Train Loss: 2.7407\n",
            "Epoch: 4, Batch: 0, Loss: 2.0291\n",
            "Epoch 4/100, Train Loss: 1.4041\n",
            "Epoch: 5, Batch: 0, Loss: 1.8671\n",
            "Epoch 5/100, Train Loss: 0.9608\n",
            "Epoch: 6, Batch: 0, Loss: 0.5901\n",
            "Epoch 6/100, Train Loss: 0.7506\n",
            "Epoch: 7, Batch: 0, Loss: 0.6819\n",
            "Epoch 7/100, Train Loss: 0.7282\n",
            "Epoch: 8, Batch: 0, Loss: 0.4440\n",
            "Epoch 8/100, Train Loss: 0.6386\n",
            "Epoch: 9, Batch: 0, Loss: 0.5160\n",
            "Epoch 9/100, Train Loss: 0.6394\n",
            "Epoch: 10, Batch: 0, Loss: 0.4163\n",
            "Epoch 10/100, Train Loss: 0.6126\n",
            "Epoch: 11, Batch: 0, Loss: 0.8849\n",
            "Epoch 11/100, Train Loss: 0.6012\n",
            "Epoch: 12, Batch: 0, Loss: 0.5571\n",
            "Epoch 12/100, Train Loss: 0.5865\n",
            "Epoch: 13, Batch: 0, Loss: 0.4890\n",
            "Epoch 13/100, Train Loss: 0.5699\n",
            "Epoch: 14, Batch: 0, Loss: 0.5147\n",
            "Epoch 14/100, Train Loss: 0.5744\n",
            "Epoch: 15, Batch: 0, Loss: 0.5904\n",
            "Epoch 15/100, Train Loss: 0.5877\n",
            "Epoch: 16, Batch: 0, Loss: 0.5458\n",
            "Epoch 16/100, Train Loss: 0.5603\n",
            "Epoch: 17, Batch: 0, Loss: 0.6569\n",
            "Epoch 17/100, Train Loss: 0.5476\n",
            "Epoch: 18, Batch: 0, Loss: 0.4948\n",
            "Epoch 18/100, Train Loss: 0.5491\n",
            "Epoch: 19, Batch: 0, Loss: 0.5723\n",
            "Epoch 19/100, Train Loss: 0.5574\n",
            "Epoch: 20, Batch: 0, Loss: 0.4429\n",
            "Epoch 20/100, Train Loss: 0.5510\n",
            "Epoch: 21, Batch: 0, Loss: 0.4678\n",
            "Epoch 21/100, Train Loss: 0.5352\n",
            "Epoch: 22, Batch: 0, Loss: 0.5463\n",
            "Epoch 22/100, Train Loss: 0.5384\n",
            "Epoch: 23, Batch: 0, Loss: 0.6276\n",
            "Epoch 23/100, Train Loss: 0.5386\n",
            "Epoch: 24, Batch: 0, Loss: 0.4121\n",
            "Epoch 24/100, Train Loss: 0.5361\n",
            "Epoch: 25, Batch: 0, Loss: 0.4899\n",
            "Epoch 25/100, Train Loss: 0.5471\n",
            "Epoch: 26, Batch: 0, Loss: 0.4486\n",
            "Epoch 26/100, Train Loss: 0.5402\n",
            "Epoch: 27, Batch: 0, Loss: 0.5583\n",
            "Epoch 27/100, Train Loss: 0.5346\n",
            "Epoch: 28, Batch: 0, Loss: 0.5702\n",
            "Epoch 28/100, Train Loss: 0.5465\n",
            "Epoch: 29, Batch: 0, Loss: 0.5802\n",
            "Epoch 29/100, Train Loss: 0.5443\n",
            "Epoch: 30, Batch: 0, Loss: 0.4616\n",
            "Epoch 30/100, Train Loss: 0.5351\n",
            "Epoch: 31, Batch: 0, Loss: 0.4988\n",
            "Epoch 31/100, Train Loss: 0.5349\n",
            "Epoch: 32, Batch: 0, Loss: 0.5927\n",
            "Epoch 32/100, Train Loss: 0.5303\n",
            "Epoch: 33, Batch: 0, Loss: 0.4679\n",
            "Epoch 33/100, Train Loss: 0.5457\n",
            "Epoch: 34, Batch: 0, Loss: 0.4879\n",
            "Epoch 34/100, Train Loss: 0.5376\n",
            "Epoch: 35, Batch: 0, Loss: 0.4591\n",
            "Epoch 35/100, Train Loss: 0.5185\n",
            "Epoch: 36, Batch: 0, Loss: 0.4411\n",
            "Epoch 36/100, Train Loss: 0.5309\n",
            "Epoch: 37, Batch: 0, Loss: 0.5065\n",
            "Epoch 37/100, Train Loss: 0.5226\n",
            "Epoch: 38, Batch: 0, Loss: 0.4720\n",
            "Epoch 38/100, Train Loss: 0.5361\n",
            "Epoch: 39, Batch: 0, Loss: 0.4748\n",
            "Epoch 39/100, Train Loss: 0.5290\n",
            "Epoch: 40, Batch: 0, Loss: 0.4355\n",
            "Epoch 40/100, Train Loss: 0.5235\n",
            "Epoch: 41, Batch: 0, Loss: 0.6537\n",
            "Epoch 41/100, Train Loss: 0.5358\n",
            "Epoch: 42, Batch: 0, Loss: 0.4951\n",
            "Epoch 42/100, Train Loss: 0.5396\n",
            "Epoch: 43, Batch: 0, Loss: 0.5902\n",
            "Epoch 43/100, Train Loss: 0.5313\n",
            "Epoch: 44, Batch: 0, Loss: 0.5429\n",
            "Epoch 44/100, Train Loss: 0.5309\n",
            "Epoch: 45, Batch: 0, Loss: 0.6061\n",
            "Epoch 45/100, Train Loss: 0.5176\n",
            "Epoch: 46, Batch: 0, Loss: 0.4948\n",
            "Epoch 46/100, Train Loss: 0.5497\n",
            "Epoch: 47, Batch: 0, Loss: 0.4509\n",
            "Epoch 47/100, Train Loss: 0.5279\n",
            "Epoch: 48, Batch: 0, Loss: 0.5992\n",
            "Epoch 48/100, Train Loss: 0.5238\n",
            "Epoch: 49, Batch: 0, Loss: 0.4827\n",
            "Epoch 49/100, Train Loss: 0.5236\n",
            "Epoch: 50, Batch: 0, Loss: 0.5989\n",
            "Epoch 50/100, Train Loss: 0.5304\n",
            "Epoch: 51, Batch: 0, Loss: 0.6089\n",
            "Epoch 51/100, Train Loss: 0.5319\n",
            "Epoch: 52, Batch: 0, Loss: 0.4679\n",
            "Epoch 52/100, Train Loss: 0.5355\n",
            "Epoch: 53, Batch: 0, Loss: 0.4445\n",
            "Epoch 53/100, Train Loss: 0.5290\n",
            "Epoch: 54, Batch: 0, Loss: 0.5461\n",
            "Epoch 54/100, Train Loss: 0.5203\n",
            "Epoch: 55, Batch: 0, Loss: 0.5340\n",
            "Epoch 55/100, Train Loss: 0.5391\n",
            "Epoch: 56, Batch: 0, Loss: 0.4234\n",
            "Epoch 56/100, Train Loss: 0.5097\n",
            "Epoch: 57, Batch: 0, Loss: 0.6034\n",
            "Epoch 57/100, Train Loss: 0.5318\n",
            "Epoch: 58, Batch: 0, Loss: 0.5266\n",
            "Epoch 58/100, Train Loss: 0.5349\n",
            "Epoch: 59, Batch: 0, Loss: 0.5776\n",
            "Epoch 59/100, Train Loss: 0.5446\n",
            "Epoch: 60, Batch: 0, Loss: 0.6041\n",
            "Epoch 60/100, Train Loss: 0.5247\n",
            "Epoch: 61, Batch: 0, Loss: 0.5794\n",
            "Epoch 61/100, Train Loss: 0.5336\n",
            "Epoch: 62, Batch: 0, Loss: 0.4587\n",
            "Epoch 62/100, Train Loss: 0.5277\n",
            "Epoch: 63, Batch: 0, Loss: 0.4880\n",
            "Epoch 63/100, Train Loss: 0.5270\n",
            "Epoch: 64, Batch: 0, Loss: 0.5560\n",
            "Epoch 64/100, Train Loss: 0.5246\n",
            "Epoch: 65, Batch: 0, Loss: 0.5666\n",
            "Epoch 65/100, Train Loss: 0.5309\n",
            "Epoch: 66, Batch: 0, Loss: 0.6047\n",
            "Epoch 66/100, Train Loss: 0.5204\n",
            "Epoch: 67, Batch: 0, Loss: 0.5586\n",
            "Epoch 67/100, Train Loss: 0.5210\n",
            "Epoch: 68, Batch: 0, Loss: 0.5218\n",
            "Epoch 68/100, Train Loss: 0.5153\n",
            "Epoch: 69, Batch: 0, Loss: 0.5682\n",
            "Epoch 69/100, Train Loss: 0.5258\n",
            "Epoch: 70, Batch: 0, Loss: 0.4475\n",
            "Epoch 70/100, Train Loss: 0.5309\n",
            "Epoch: 71, Batch: 0, Loss: 0.4581\n",
            "Epoch 71/100, Train Loss: 0.5296\n",
            "Epoch: 72, Batch: 0, Loss: 0.4889\n",
            "Epoch 72/100, Train Loss: 0.5335\n",
            "Epoch: 73, Batch: 0, Loss: 0.4509\n",
            "Epoch 73/100, Train Loss: 0.5213\n",
            "Epoch: 74, Batch: 0, Loss: 0.4361\n",
            "Epoch 74/100, Train Loss: 0.5259\n",
            "Epoch: 75, Batch: 0, Loss: 0.5837\n",
            "Epoch 75/100, Train Loss: 0.5285\n",
            "Epoch: 76, Batch: 0, Loss: 0.5096\n",
            "Epoch 76/100, Train Loss: 0.5213\n",
            "Epoch: 77, Batch: 0, Loss: 0.5462\n",
            "Epoch 77/100, Train Loss: 0.5144\n",
            "Epoch: 78, Batch: 0, Loss: 0.5497\n",
            "Epoch 78/100, Train Loss: 0.5380\n",
            "Epoch: 79, Batch: 0, Loss: 0.6053\n",
            "Epoch 79/100, Train Loss: 0.5245\n",
            "Epoch: 80, Batch: 0, Loss: 0.5191\n",
            "Epoch 80/100, Train Loss: 0.5223\n",
            "Epoch: 81, Batch: 0, Loss: 0.5681\n",
            "Epoch 81/100, Train Loss: 0.5292\n",
            "Epoch: 82, Batch: 0, Loss: 0.6497\n",
            "Epoch 82/100, Train Loss: 0.5400\n",
            "Epoch: 83, Batch: 0, Loss: 0.5942\n",
            "Epoch 83/100, Train Loss: 0.5244\n",
            "Epoch: 84, Batch: 0, Loss: 0.5839\n",
            "Epoch 84/100, Train Loss: 0.5340\n",
            "Epoch: 85, Batch: 0, Loss: 0.5878\n",
            "Epoch 85/100, Train Loss: 0.5361\n",
            "Epoch: 86, Batch: 0, Loss: 0.5162\n",
            "Epoch 86/100, Train Loss: 0.5216\n",
            "Epoch: 87, Batch: 0, Loss: 0.4604\n",
            "Epoch 87/100, Train Loss: 0.5252\n",
            "Epoch: 88, Batch: 0, Loss: 0.5503\n",
            "Epoch 88/100, Train Loss: 0.5293\n",
            "Epoch: 89, Batch: 0, Loss: 0.5519\n",
            "Epoch 89/100, Train Loss: 0.5207\n",
            "Epoch: 90, Batch: 0, Loss: 0.4734\n",
            "Epoch 90/100, Train Loss: 0.5187\n",
            "Epoch: 91, Batch: 0, Loss: 0.4472\n",
            "Epoch 91/100, Train Loss: 0.5238\n",
            "Epoch: 92, Batch: 0, Loss: 0.5800\n",
            "Epoch 92/100, Train Loss: 0.5366\n",
            "Epoch: 93, Batch: 0, Loss: 0.4380\n",
            "Epoch 93/100, Train Loss: 0.5159\n",
            "Epoch: 94, Batch: 0, Loss: 0.5534\n",
            "Epoch 94/100, Train Loss: 0.5408\n",
            "Epoch: 95, Batch: 0, Loss: 0.4630\n",
            "Epoch 95/100, Train Loss: 0.5223\n",
            "Epoch: 96, Batch: 0, Loss: 0.4323\n",
            "Epoch 96/100, Train Loss: 0.5309\n",
            "Epoch: 97, Batch: 0, Loss: 0.5814\n",
            "Epoch 97/100, Train Loss: 0.5243\n",
            "Epoch: 98, Batch: 0, Loss: 0.5194\n",
            "Epoch 98/100, Train Loss: 0.5260\n",
            "Epoch: 99, Batch: 0, Loss: 0.5867\n",
            "Epoch 99/100, Train Loss: 0.5363\n",
            "Epoch: 100, Batch: 0, Loss: 0.4878\n",
            "Epoch 100/100, Train Loss: 0.5395\n",
            "\n",
            "==================================================\n",
            "ТЕСТИРОВАНИЕ МОДЕЛИ\n",
            "==================================================\n",
            "В: какая столица франции\n",
            "О: python это сеть соединенных компьютеров\n",
            "\n",
            "В: как работает фотосинтез\n",
            "О: ешьте здоровую пищу и занимайтесь спортом\n",
            "\n",
            "В: что такое черная дыра\n",
            "О: мо это алгоритмы обучающиеся на данных\n",
            "\n",
            "Модель сохранена в 'qa_model.pth'\n"
          ]
        }
      ]
    }
  ]
}
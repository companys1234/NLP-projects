{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Создаём collator с динамическим маскированием\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15\n",
        ")\n",
        "\n",
        "# Токенизируем ТЕКСТЫ (отдельно, без return_tensors)\n",
        "texts = [\"I love machine learning\", \"Transformers are powerful\"]\n",
        "tokenized = [tokenizer(t, return_tensors=None) for t in texts]\n",
        "\n",
        "# Collator применяет динамическое маскирование\n",
        "batch = data_collator(tokenized)\n",
        "\n",
        "print(\"input_ids:\\n\", batch[\"input_ids\"])\n",
        "print(\"labels:\\n\", batch[\"labels\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaKcdluVKwMo",
        "outputId": "81dcc22c-c910-49e1-d228-1e983d081a72"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids:\n",
            " tensor([[    0, 50264,   657, 50264,  2239,     2],\n",
            "        [    0, 44820,   268,    32, 50264,     2]])\n",
            "labels:\n",
            " tensor([[-100,  100, -100, 3563, -100, -100],\n",
            "        [-100, -100, -100, -100, 2247, -100]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM_SLLwxHPrS",
        "outputId": "c7ff95a7-2022-48c6-8024-ce4be1d7bba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits.shape: torch.Size([2, 16, 50265])\n"
          ]
        }
      ],
      "source": [
        "# robeta_architecture.py\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RobertaConfig:\n",
        "    vocab_size: int = 50265\n",
        "    hidden_size: int = 768\n",
        "    num_hidden_layers: int = 12\n",
        "    num_attention_heads: int = 12\n",
        "    intermediate_size: int = 3072\n",
        "    hidden_act: str = \"gelu\"\n",
        "    hidden_dropout_prob: float = 0.1\n",
        "    attention_probs_dropout_prob: float = 0.1\n",
        "    max_position_embeddings: int = 514  # RoBERTa often uses 514 (including <s>, </s>)\n",
        "    type_vocab_size: int = 1  # RoBERTa не использует сегментные эмбеддинги, но оставим параметр\n",
        "    layer_norm_eps: float = 1e-12\n",
        "    initializer_range: float = 0.02\n",
        "    pad_token_id: int = 1\n",
        "\n",
        "\n",
        "def get_activation(name: str):\n",
        "    if name in (\"gelu\", \"gelu_new\"):\n",
        "        return F.gelu\n",
        "    if name == \"relu\":\n",
        "        return F.relu\n",
        "    if name == \"swish\":\n",
        "        return lambda x: x * torch.sigmoid(x)\n",
        "    raise ValueError(f\"Unknown activation: {name}\")\n",
        "\n",
        "\n",
        "class RobertaEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Token embeddings + position embeddings.\n",
        "    RoBERTa: нет сегментных эмбеддингов (type_embeddings обычно отключены).\n",
        "    \"\"\"\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        # RoBERTa doesn't use token_type embeddings, but keep for API-compatibility\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # Initialize positions 0..max_position-1 as usual\n",
        "        self._reset_parameters(config)\n",
        "\n",
        "    def _reset_parameters(self, config: RobertaConfig):\n",
        "        nn.init.normal_(self.word_embeddings.weight, mean=0.0, std=config.initializer_range)\n",
        "        nn.init.normal_(self.position_embeddings.weight, mean=0.0, std=config.initializer_range)\n",
        "        if config.type_vocab_size > 0:\n",
        "            nn.init.normal_(self.token_type_embeddings.weight, mean=0.0, std=config.initializer_range)\n",
        "\n",
        "    def forward(self, input_ids: torch.LongTensor, position_ids: Optional[torch.LongTensor] = None, token_type_ids: Optional[torch.LongTensor] = None):\n",
        "        seq_length = input_ids.size(1)\n",
        "        if position_ids is None:\n",
        "            # positions start at 0\n",
        "            device = input_ids.device\n",
        "            position_ids = torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        words = self.word_embeddings(input_ids)\n",
        "        positions = self.position_embeddings(position_ids)\n",
        "        types = self.token_type_embeddings(token_type_ids) if self.token_type_embeddings.num_embeddings > 0 else 0\n",
        "\n",
        "        embeddings = words + positions + types\n",
        "        embeddings = self.layernorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class RobertaSelfAttention(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\"hidden_size must be divisible by num_attention_heads\")\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "        self._reset_parameters(config)\n",
        "\n",
        "    def _reset_parameters(self, config: RobertaConfig):\n",
        "        nn.init.normal_(self.query.weight, mean=0.0, std=config.initializer_range)\n",
        "        nn.init.normal_(self.key.weight, mean=0.0, std=config.initializer_range)\n",
        "        nn.init.normal_(self.value.weight, mean=0.0, std=config.initializer_range)\n",
        "        if self.query.bias is not None:\n",
        "            nn.init.zeros_(self.query.bias)\n",
        "            nn.init.zeros_(self.key.bias)\n",
        "            nn.init.zeros_(self.value.bias)\n",
        "\n",
        "    def transpose_for_scores(self, x: torch.Tensor):\n",
        "        # x: [batch, seq_len, all_head_size] -> [batch, num_heads, seq_len, head_size]\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, head_mask: Optional[torch.Tensor] = None):\n",
        "        # hidden_states: [batch, seq_len, hidden_size]\n",
        "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
        "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        # Attention scores: [batch, heads, seq_len, seq_len]\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # attention_mask should be broadcastable to [batch, heads, seq_len, seq_len]\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # apply head_mask if provided (usually None)\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)  # [batch, heads, seq_len, head_size]\n",
        "        # back to [batch, seq_len, all_head_size]\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_shape)\n",
        "        return context_layer, attention_probs\n",
        "\n",
        "\n",
        "class RobertaSelfOutput(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        nn.init.normal_(self.dense.weight, mean=0.0, std=config.initializer_range)\n",
        "        if self.dense.bias is not None:\n",
        "            nn.init.zeros_(self.dense.bias)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.layernorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class RobertaAttention(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.self = RobertaSelfAttention(config)\n",
        "        self.output = RobertaSelfOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, head_mask: Optional[torch.Tensor] = None):\n",
        "        self_outputs = self.self(hidden_states, attention_mask, head_mask)\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        return attention_output, self_outputs[1]  # (output, attention_probs)\n",
        "\n",
        "\n",
        "class RobertaIntermediate(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = get_activation(config.hidden_act)\n",
        "        nn.init.normal_(self.dense.weight, mean=0.0, std=config.initializer_range)\n",
        "        if self.dense.bias is not None:\n",
        "            nn.init.zeros_(self.dense.bias)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class RobertaOutput(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        nn.init.normal_(self.dense.weight, mean=0.0, std=config.initializer_range)\n",
        "        if self.dense.bias is not None:\n",
        "            nn.init.zeros_(self.dense.bias)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.layernorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class RobertaLayer(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.attention = RobertaAttention(config)\n",
        "        self.intermediate = RobertaIntermediate(config)\n",
        "        self.output = RobertaOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, head_mask: Optional[torch.Tensor] = None):\n",
        "        attention_output, attn_probs = self.attention(hidden_states, attention_mask, head_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output, attn_probs\n",
        "\n",
        "\n",
        "class RobertaEncoder(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False, output_hidden_states: bool = False):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i] if head_mask is not None else None)\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        # final\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        outputs = (hidden_states, )\n",
        "        if output_hidden_states:\n",
        "            outputs = outputs + (all_hidden_states, )\n",
        "        if output_attentions:\n",
        "            outputs = outputs + (all_attentions, )\n",
        "        return outputs  # (last_hidden_state, optional all_hidden_states, optional all_attentions)\n",
        "\n",
        "\n",
        "class RobertaModel(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = RobertaEmbeddings(config)\n",
        "        self.encoder = RobertaEncoder(config)\n",
        "        # final layer_norm as in some implementations (optional)\n",
        "        self.pooler = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.pooler_activation = nn.Tanh()\n",
        "        self._init_weights(config)\n",
        "\n",
        "    def _init_weights(self, config: RobertaConfig):\n",
        "        nn.init.normal_(self.pooler.weight, mean=0.0, std=config.initializer_range)\n",
        "        if self.pooler.bias is not None:\n",
        "            nn.init.zeros_(self.pooler.bias)\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def set_input_embeddings(self, new_embeddings):\n",
        "        self.embeddings.word_embeddings = new_embeddings\n",
        "\n",
        "    def forward(self, input_ids: torch.LongTensor, attention_mask: Optional[torch.Tensor] = None, token_type_ids: Optional[torch.Tensor] = None, position_ids: Optional[torch.Tensor] = None, output_attentions: bool = False, output_hidden_states: bool = False):\n",
        "        # attention_mask: [batch, seq_len] with 1 for tokens to attend, 0 for pad\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "        # Convert attention mask to the shape [batch, 1, 1, seq_len] with 0.0 for keep and -10000.0 for mask (additive)\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [B,1,1,S]\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\n",
        "        encoder_outputs = self.encoder(embedding_output, extended_attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states)\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output[:, 0])  # use first token (CLS)\n",
        "        pooled_output = self.pooler_activation(pooled_output)\n",
        "\n",
        "        outputs = (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "        return outputs  # (sequence_output, pooled_output, optional hidden_states, optional attentions)\n",
        "\n",
        "\n",
        "class RobertaLMHead(nn.Module):\n",
        "    \"\"\" MLM head: maps hidden states to vocabulary logits (tied to input embeddings optionally) \"\"\"\n",
        "    def __init__(self, config: RobertaConfig, embedding_weights: Optional[nn.Embedding] = None):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = get_activation(config.hidden_act)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "        if embedding_weights is not None:\n",
        "            # weight tying\n",
        "            self.decoder.weight = embedding_weights.weight\n",
        "\n",
        "        nn.init.normal_(self.dense.weight, mean=0.0, std=config.initializer_range)\n",
        "        if self.dense.bias is not None:\n",
        "            nn.init.zeros_(self.dense.bias)\n",
        "        nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor):\n",
        "        x = self.dense(hidden_states)\n",
        "        x = self.activation(x)\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.decoder(x) + self.bias\n",
        "        return x\n",
        "\n",
        "\n",
        "class RobertaForMaskedLM(nn.Module):\n",
        "    def __init__(self, config: RobertaConfig):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.lm_head = RobertaLMHead(config, embedding_weights=self.roberta.get_input_embeddings())\n",
        "        # tie weights already done in LMHead init\n",
        "\n",
        "    def forward(self, input_ids: torch.LongTensor, attention_mask: Optional[torch.Tensor] = None, labels: Optional[torch.LongTensor] = None):\n",
        "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.lm_head(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # labels: [batch, seq_len], ignore_index = -100\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_fct(prediction_scores.view(-1, self.roberta.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        return (loss, prediction_scores) if loss is not None else (prediction_scores,)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cfg = RobertaConfig()\n",
        "    model = RobertaForMaskedLM(cfg)\n",
        "    batch_size = 2\n",
        "    seq_len = 16\n",
        "    # случайные input_ids (значения 0..vocab_size-1)\n",
        "    input_ids = torch.randint(0, cfg.vocab_size, (batch_size, seq_len), dtype=torch.long)\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    logits = model(input_ids, attention_mask=attention_mask)[0]\n",
        "    print(\"logits.shape:\", logits.shape)  # [batch, seq_len, vocab_size]\n"
      ]
    }
  ]
}